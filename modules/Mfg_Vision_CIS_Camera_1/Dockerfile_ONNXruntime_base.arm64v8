# syntax=docker/dockerfile:experimental
#
# This Dockerfile installs the pre-built ONNX Runtime wheel inside the image.
# Please make sure you have nvidia-runtime enabled in docker config and then build on your Jetson-based device:
#
# sudo -H DOCKER_BUILDKIT=1 nvidia-docker build -f Dockerfile.jetson -t aiotdevopsacr.azurecr.io/onnxruntime_jp451:1.8.0.2-arm64v8 . && sudo docker push aiotdevopsacr.azurecr.io/onnxruntime_jp451:1.8.0.2-arm64v8
#

ARG BASE_IMAGE=nvcr.io/nvidia/l4t-base:r32.5.0
FROM ${BASE_IMAGE} as onnxruntime

ARG WHEEL_FILE=onnxruntime_gpu-1.8.0-cp36-cp36m-linux_aarch64.whl

# Ensure apt-get won't prompt for selecting options
ENV DEBIAN_FRONTEND=noninteractive

RUN apt update && \
    apt install -y --no-install-recommends \
    	build-essential \
        software-properties-common \
        libopenblas-dev \
        libpython3.6-dev \
        python3-pip \
        python3-dev \
        cmake \
	    mlocate \
        unattended-upgrades
RUN unattended-upgrade
RUN updatedb
RUN pip3 install --upgrade pip setuptools wheel
RUN pip3 install pybind11 pytest

ENV OPENBLAS_CORETYPE=ARMV8:$OPENBLAS_CORETYPE
ENV LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libgomp.so.1
ENV LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libcublas.so.10
ENV LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64:$LD_LIBRARY_PATH
ENV LD_LIBRARY_PATH=/usr/lib/aarch64-linux-gnu:$LD_LIBRARY_PATH
ENV PATH=/usr/local/cuda-10.2/bin:$PATH
ENV PATH=/usr/lib/aarch64-linux-gnu:$PATH
ENV CUDA_PATH=/usr/local/cuda-10.2:$CUDA_PATH
ENV cuDNN_PATH=/usr/lib/aarch64-linux-gnu$cuDNN_PATH
RUN ldconfig

WORKDIR /onnxruntime

# copy previously built wheel into the container
COPY ${WHEEL_FILE} .

RUN basename ${WHEEL_FILE} | xargs pip3 install 